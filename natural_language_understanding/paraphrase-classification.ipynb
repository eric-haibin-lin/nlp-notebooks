{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase Classification by Fine-tuning BERT\n",
    "\n",
    "Pre-trained language\n",
    "representations have been shown to improve many downstream NLP tasks such as\n",
    "question answering, and natural language inference. To apply pre-trained\n",
    "representations to these tasks, there are two strategies:\n",
    "\n",
    "1. **feature-based**\n",
    "approach, which uses the pre-trained representations as additional\n",
    "features to\n",
    "the downstream task.\n",
    "2. **fine-tuning** based approach, which trains the\n",
    "downstream tasks by\n",
    "fine-tuning pre-trained parameters.\n",
    "\n",
    "While feature-based\n",
    "approaches such as ELMo [3] (introduced in the previous tutorial) are effective\n",
    "in improving many downstream tasks, they require task-specific architectures.\n",
    "Devlin, Jacob, et al proposed BERT [1] (Bidirectional Encoder Representations\n",
    "from Transformers), which **fine-tunes** deep bidirectional representations on a\n",
    "wide range of tasks with minimal task-specific parameters, and obtained state-\n",
    "of-the-art results.\n",
    "\n",
    "In this tutorial, we will focus on fine-tuning with the\n",
    "pre-trained BERT model to classify semantically equivalent sentence pairs.\n",
    "Specifically, we will:\n",
    "\n",
    "1. load the state-of-the-art pre-trained BERT model and\n",
    "attach an additional layer for classification,\n",
    "2. process and transform sentence\n",
    "pair data for the task at hand, and\n",
    "3. fine-tune BERT model for sentence\n",
    "classification.\n",
    "\n",
    "## Preparation\n",
    "\n",
    "To run this tutorial locally, please:\n",
    "\n",
    "- [install gluonnlp](http://gluon-nlp.mxnet.io/#installation), and\n",
    "- click the\n",
    "download button at the top of the tutorial page to get all related code.\n",
    "\n",
    "Then\n",
    "we start with some usual preparation such as importing libraries\n",
    "and setting the\n",
    "environment.\n",
    "\n",
    "### Load MXNet and GluonNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import io\n",
    "import random\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "from bert import data, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "# change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Pre-trained BERT Model\n",
    "\n",
    "The list of pre-trained BERT model available\n",
    "in GluonNLP can be found\n",
    "[here](../../model_zoo/bert/index.rst).\n",
    "\n",
    "In this\n",
    "tutorial, we will load the BERT\n",
    "BASE model trained on uncased book corpus and\n",
    "English Wikipedia dataset in\n",
    "GluonNLP model zoo.\n",
    "\n",
    "### Get BERT\n",
    "\n",
    "Let's first take\n",
    "a look at the BERT model\n",
    "architecture for sentence pair classification below:\n",
    "<div style=\"width:\n",
    "500px;\">![bert-sentence-pair](bert-sentence-pair.png)</div>\n",
    "where the model takes a pair of\n",
    "sequences and *pools* the representation of the\n",
    "first token in the sequence.\n",
    "Note that the original BERT model was trained for\n",
    "masked language model and next\n",
    "sentence prediction tasks, which includes layers\n",
    "for language model decoding and\n",
    "classification. These layers will not be used\n",
    "for fine-tuning sentence pair classification.\n",
    "\n",
    "Let's load the\n",
    "pre-trained BERT\n",
    "using the model API in GluonNLP, which returns the vocabulary\n",
    "along with the\n",
    "model. We include the pooler layer of the pre-trained model by setting\n",
    "`use_pooler` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTModel(\n",
      "  (encoder): BERTEncoder(\n",
      "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "    (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (1): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (2): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (3): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (4): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (5): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (6): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (7): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (8): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (9): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (10): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (11): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_embed): HybridSequential(\n",
      "    (0): Embedding(30522 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (token_type_embed): HybridSequential(\n",
      "    (0): Embedding(2 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (pooler): Dense(768 -> 768, Activation(tanh))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bert_base, vocabulary = nlp.model.get_model('bert_12_768_12', \n",
    "                                             dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                             pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                             use_decoder=False, use_classifier=False)\n",
    "print(bert_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition for Sentence Pair Classification\n",
    "\n",
    "Now that we have loaded\n",
    "the BERT model, we only need to attach an additional layer for classification.\n",
    "The `BERTClassifier` class uses a BERT base model to encode sentence\n",
    "representation, followed by a `nn.Dense` layer for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.classification.BERTClassifier(bert_base, num_classes=2, dropout=0.1)\n",
    "# only need to initialize the classifier layer.\n",
    "model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "model.hybridize(static_alloc=True)\n",
    "\n",
    "# softmax cross entropy loss for classification\n",
    "loss_function = mx.gluon.loss.SoftmaxCELoss()\n",
    "loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "metric = mx.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing for BERT\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "For demonstration purpose, we use\n",
    "the dev set of the\n",
    "Microsoft Research Paraphrase Corpus dataset. The file is\n",
    "named 'dev.tsv'. Let's take a look at the first few lines of the raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿Quality\t#1 ID\t#2 ID\t#1 String\t#2 String\n",
      "\n",
      "1\t1355540\t1355592\tHe said the foodservice pie business doesn 't fit the company 's long-term growth strategy .\t\" The foodservice pie business does not fit our long-term growth strategy .\n",
      "\n",
      "0\t2029631\t2029565\tMagnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war .\tHis wife said he was \" 100 percent behind George Bush \" and looked forward to using his years of training in the war .\n",
      "\n",
      "0\t487993\t487952\tThe dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat .\tThe dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .\n",
      "\n",
      "1\t1989515\t1989458\tThe AFL-CIO is waiting until October to decide if it will endorse a candidate .\tThe AFL-CIO announced Wednesday that it will decide in October whether to endorse a candidate before the primaries .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tsv_file = io.open('dev.tsv', encoding='utf-8')\n",
    "for i in range(5):\n",
    "    print(tsv_file.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains 5 columns, separated by tabs.\n",
    "The first line of\n",
    "the file explains each of these columns:\n",
    "0. the label indicating whether the two\n",
    "sentences are semantically equivalent\n",
    "1. the id of the first sentence in this\n",
    "sample\n",
    "2. the id of the second sentence in this sample\n",
    "3. the content of the\n",
    "first sentence\n",
    "4. the content of the second sentence\n",
    "\n",
    "For our task, we are\n",
    "interested in the 0th, 3rd and 4th columns.\n",
    "To load this dataset, we can use the\n",
    "`TSVDataset` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He said the foodservice pie business doesn 't fit the company 's long-term growth strategy .\n",
      "\" The foodservice pie business does not fit our long-term growth strategy .\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# skip the first line, which is the schema\n",
    "num_discard_samples = 1\n",
    "# split fields by tabs\n",
    "field_separator = nlp.data.Splitter('\\t')\n",
    "# fields to select from the file\n",
    "field_indices = [3, 4, 0]\n",
    "data_train_raw = nlp.data.TSVDataset(filename='dev.tsv',\n",
    "                                 field_separator=field_separator,\n",
    "                                 num_discard_samples=num_discard_samples,\n",
    "                                 field_indices=field_indices)\n",
    "sample_id = 0\n",
    "# sentence a\n",
    "print(data_train_raw[sample_id][0])\n",
    "# sentence b\n",
    "print(data_train_raw[sample_id][1])\n",
    "# 1 means equivalent, 0 means not equivalent\n",
    "print(data_train_raw[sample_id][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the pre-trained BERT model, we need to preprocess the data in the same\n",
    "way it was trained. The following figure shows the input representation in BERT:\n",
    "<div style=\"width: 500px;\">![bert-embed](bert-embed.png)</div>\n",
    "\n",
    "We will use\n",
    "`BERTDatasetTransform` to perform the following transformations:\n",
    "- tokenize\n",
    "the\n",
    "input sequences\n",
    "- insert [CLS] at the beginning\n",
    "- insert [SEP] between sentence\n",
    "one and sentence two, and at the end\n",
    "- generate segment ids to indicate whether\n",
    "a token belongs to the first sequence or the second sequence.\n",
    "- generate valid\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n",
      "token ids = \n",
      "[    2  2002  2056  1996  9440  2121  7903  2063 11345  2449  2987  1005\n",
      "  1056  4906  1996  2194  1005  1055  2146  1011  2744  3930  5656  1012\n",
      "     3  1000  1996  9440  2121  7903  2063 11345  2449  2515  2025  4906\n",
      "  2256  2146  1011  2744  3930  5656  1012     3     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "valid length = \n",
      "44\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "label = \n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# use the vocabulary from pre-trained model for tokenization\n",
    "bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "# maximum sequence length\n",
    "max_len = 128\n",
    "# the labels for the two classes\n",
    "all_labels = [\"0\", \"1\"]\n",
    "# whether to transform the data as sentence pairs.\n",
    "# for single sentence classification, set pair=False\n",
    "# for regression task, set class_labels=None\n",
    "# for inference without label available, set has_label=False\n",
    "pair = True\n",
    "transform = data.transform.BERTDatasetTransform(bert_tokenizer, max_len,\n",
    "                                                class_labels=all_labels,\n",
    "                                                has_label=True,\n",
    "                                                pad=True,\n",
    "                                                pair=pair)\n",
    "data_train = data_train_raw.transform(transform)\n",
    "\n",
    "print('vocabulary used for tokenization = \\n%s'%vocabulary)\n",
    "print('%s token id = %s'%(vocabulary.padding_token, vocabulary[vocabulary.padding_token]))\n",
    "print('%s token id = %s'%(vocabulary.cls_token, vocabulary[vocabulary.cls_token]))\n",
    "print('%s token id = %s'%(vocabulary.sep_token, vocabulary[vocabulary.sep_token]))\n",
    "print('token ids = \\n%s'%data_train[sample_id][0])\n",
    "print('valid length = \\n%s'%data_train[sample_id][1])\n",
    "print('segment ids = \\n%s'%data_train[sample_id][2])\n",
    "print('label = \\n%s'%data_train[sample_id][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune BERT Model\n",
    "\n",
    "Putting everything together, now we can fine-tune the\n",
    "model with a few epochs. For demonstration, we use a fixed learning rate and\n",
    "skip validation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 4/31] loss=0.7040, lr=0.0000500, acc=0.542\n",
      "[Epoch 0 Batch 8/31] loss=0.7539, lr=0.0000500, acc=0.604\n",
      "[Epoch 0 Batch 12/31] loss=0.5322, lr=0.0000500, acc=0.638\n",
      "[Epoch 0 Batch 16/31] loss=0.5775, lr=0.0000500, acc=0.653\n",
      "[Epoch 0 Batch 20/31] loss=0.7571, lr=0.0000500, acc=0.646\n",
      "[Epoch 0 Batch 24/31] loss=0.6011, lr=0.0000500, acc=0.658\n",
      "[Epoch 0 Batch 28/31] loss=0.6654, lr=0.0000500, acc=0.647\n",
      "[Epoch 1 Batch 4/31] loss=0.4305, lr=0.0000500, acc=0.740\n",
      "[Epoch 1 Batch 8/31] loss=0.6095, lr=0.0000500, acc=0.723\n",
      "[Epoch 1 Batch 12/31] loss=0.6919, lr=0.0000500, acc=0.677\n",
      "[Epoch 1 Batch 16/31] loss=0.5523, lr=0.0000500, acc=0.696\n",
      "[Epoch 1 Batch 20/31] loss=0.4112, lr=0.0000500, acc=0.700\n",
      "[Epoch 1 Batch 24/31] loss=0.6507, lr=0.0000500, acc=0.684\n",
      "[Epoch 1 Batch 28/31] loss=0.6448, lr=0.0000500, acc=0.683\n",
      "[Epoch 2 Batch 4/31] loss=0.3918, lr=0.0000500, acc=0.791\n",
      "[Epoch 2 Batch 8/31] loss=0.6558, lr=0.0000500, acc=0.705\n",
      "[Epoch 2 Batch 12/31] loss=0.2092, lr=0.0000500, acc=0.731\n",
      "[Epoch 2 Batch 16/31] loss=0.2908, lr=0.0000500, acc=0.773\n",
      "[Epoch 2 Batch 20/31] loss=0.3309, lr=0.0000500, acc=0.794\n",
      "[Epoch 2 Batch 24/31] loss=0.2151, lr=0.0000500, acc=0.816\n",
      "[Epoch 2 Batch 28/31] loss=0.5329, lr=0.0000500, acc=0.811\n",
      "[Epoch 3 Batch 4/31] loss=0.0881, lr=0.0000500, acc=1.000\n",
      "[Epoch 3 Batch 8/31] loss=0.3103, lr=0.0000500, acc=0.951\n",
      "[Epoch 3 Batch 12/31] loss=0.3369, lr=0.0000500, acc=0.907\n",
      "[Epoch 3 Batch 16/31] loss=0.1569, lr=0.0000500, acc=0.915\n",
      "[Epoch 3 Batch 20/31] loss=0.1437, lr=0.0000500, acc=0.919\n",
      "[Epoch 3 Batch 24/31] loss=0.2128, lr=0.0000500, acc=0.922\n",
      "[Epoch 3 Batch 28/31] loss=0.1774, lr=0.0000500, acc=0.927\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "lr = 5e-5\n",
    "train_sampler = nlp.data.FixedBucketSampler(lengths=[int(item[1]) for item in data_train],\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "bert_dataloader = mx.gluon.data.DataLoader(data_train, batch_sampler=train_sampler)\n",
    "\n",
    "trainer = mx.gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': lr})\n",
    "\n",
    "# collect all differentiable parameters\n",
    "# grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "# the gradients for these params are clipped later\n",
    "params = [p for p in model.collect_params().values() if p.grad_req != 'null']\n",
    "grad_clip = 1\n",
    "\n",
    "log_interval = 4\n",
    "num_epochs = 4\n",
    "for epoch_id in range(num_epochs):\n",
    "    metric.reset()\n",
    "    step_loss = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(bert_dataloader):\n",
    "        with mx.autograd.record():\n",
    "            \n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "            \n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "        # backward computation\n",
    "        ls.backward()\n",
    "        \n",
    "        # gradient clipping\n",
    "        trainer.allreduce_grads()\n",
    "        nlp.utils.clip_grad_global_norm(params, 1)\n",
    "        trainer.update(1)\n",
    "\n",
    "        step_loss += ls.asscalar()\n",
    "        metric.update([label], [out])\n",
    "        if (batch_id + 1) % (log_interval) == 0:\n",
    "            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}'\n",
    "                         .format(epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                 step_loss / log_interval,\n",
    "                                 trainer.learning_rate, metric.get()[1]))\n",
    "            step_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we show how to fine-tune a sentence pair\n",
    "classification model with pre-trained BERT parameters. In GluonNLP, this can be\n",
    "done with just a few simple steps: apply BERT-style data transformation to\n",
    "preprocess the data, automatically download the pre-trained model, and feed the\n",
    "transformed data into the model. For demonstration purpose, we skipped the\n",
    "warmup learning rate\n",
    "schedule and validation on dev dataset used in the original\n",
    "implementation. Please visit the\n",
    "[BERT model zoo webpage](../../model_zoo/bert/index.rst), or the scripts/bert folder\n",
    "in the Github repository for complete fine-tuning scripts.\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Devlin, Jacob, et al. \"Bert:\n",
    "Pre-training of deep\n",
    "bidirectional transformers for language understanding.\"\n",
    "arXiv preprint\n",
    "arXiv:1810.04805 (2018).\n",
    "\n",
    "[2] Dolan, William B., and Chris\n",
    "Brockett.\n",
    "\"Automatically constructing a corpus of sentential paraphrases.\"\n",
    "Proceedings of\n",
    "the Third International Workshop on Paraphrasing (IWP2005). 2005.\n",
    "[3] Peters,\n",
    "Matthew E., et al. \"Deep contextualized word representations.\" arXiv\n",
    "preprint\n",
    "arXiv:1802.05365 (2018)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_nlp",
   "language": "python",
   "name": "conda_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
